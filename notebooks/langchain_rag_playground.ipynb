{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceeac48",
   "metadata": {},
   "source": [
    "\n",
    "# LangChain RAG Playground for the `data/` Directory\n",
    "\n",
    "This notebook sets up a minimal retrieval-augmented generation (RAG) workflow powered by LangChain. It ingests the resources stored in `../data` (a PDF research paper and a Chinese-language text file), builds a vector index, and exposes a helper function for interactive question answering.\n",
    "\n",
    "> ℹ️ Feel free to clone and extend these building blocks—swap in another LLM, persist the vector store, or wire the retriever into an application backend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a337b6",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "Uncomment and run the cell below if the required packages are not yet installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -q --upgrade langchain langchain-community langchain-openai docling #     chromadb modelscope pypdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9b735",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports and Configuration\n",
    "\n",
    "We keep paths relative so the notebook works whether it is launched from the project root or the `notebooks/` directory. The loader for the Chinese text file tries several common encodings before failing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional\n",
    "import re\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ModelScopeEmbeddings\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# If you plan to use OpenAI, make sure langchain-openai is installed and set OPENAI_API_KEY.\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "except ImportError:\n",
    "    ChatOpenAI = None\n",
    "\n",
    "PROJECT_ROOT_CANDIDATES = [Path.cwd(), Path.cwd().parent]\n",
    "for candidate in PROJECT_ROOT_CANDIDATES:\n",
    "    data_dir = candidate / \"data\"\n",
    "    if data_dir.exists():\n",
    "        PROJECT_ROOT = candidate\n",
    "        DATA_DIR = data_dir\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not locate the 'data' directory. Update the path resolution logic above.\")\n",
    "\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n",
    "print(f\"Available data files: {[p.name for p in DATA_DIR.iterdir()]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea420b",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Load Raw Documents\n",
    "\n",
    "Docling handles both PDF and rich-text parsing so we can derive structured Markdown representations. We still keep a manual encoding fallback for plain text files as a last resort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENCODING_CANDIDATES: Iterable[str] = (\"utf-8\", \"utf-8-sig\", \"gb18030\", \"iso-8859-1\")\n",
    "\n",
    "DOC_CONVERTER = DocumentConverter()\n",
    "SECTION_HEADING_RE = re.compile(r\"^(#{1,6})\\s+(.*)\")\n",
    "\n",
    "\n",
    "def load_text_with_fallback(path: Path, encodings: Iterable[str] = ENCODING_CANDIDATES) -> Document:\n",
    "    last_error: Optional[Exception] = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            text = path.read_text(encoding=enc)\n",
    "            metadata = {\"source\": str(path.relative_to(PROJECT_ROOT)), \"encoding\": enc}\n",
    "            return Document(page_content=text, metadata=metadata)\n",
    "        except (UnicodeDecodeError, LookupError) as err:\n",
    "            last_error = err\n",
    "    if last_error is not None:\n",
    "        raise last_error\n",
    "    raise UnicodeDecodeError(\"Unable to decode\", b\"\", 0, 0, \"No encoding candidates were successful\")\n",
    "\n",
    "\n",
    "def iter_markdown_sections(markdown: str):\n",
    "    current_heading: Optional[str] = None\n",
    "    current_buffer: list[str] = []\n",
    "    for line in markdown.splitlines():\n",
    "        match = SECTION_HEADING_RE.match(line)\n",
    "        if match:\n",
    "            if current_buffer:\n",
    "                yield current_heading, \"\n",
    "\".join(current_buffer).strip()\n",
    "                current_buffer = []\n",
    "            current_heading = match.group(2).strip()\n",
    "        else:\n",
    "            current_buffer.append(line)\n",
    "    if current_buffer:\n",
    "        yield current_heading, \"\n",
    "\".join(current_buffer).strip()\n",
    "\n",
    "\n",
    "def chunk_section_text(text: str, max_chars: int = 1200):\n",
    "    paragraphs = [para.strip() for para in re.split(r\"\n",
    "\\s*\n",
    "\", text) if para.strip()]\n",
    "    if not paragraphs:\n",
    "        cleaned = text.strip()\n",
    "        if cleaned:\n",
    "            yield cleaned\n",
    "        return\n",
    "    buffer: list[str] = []\n",
    "    length = 0\n",
    "    for para in paragraphs:\n",
    "        para_len = len(para)\n",
    "        if buffer and length + para_len > max_chars:\n",
    "            yield \"\n",
    "\n",
    "\".join(buffer)\n",
    "            buffer = [para]\n",
    "            length = para_len\n",
    "        else:\n",
    "            buffer.append(para)\n",
    "            length += para_len\n",
    "    if buffer:\n",
    "        yield \"\n",
    "\n",
    "\".join(buffer)\n",
    "\n",
    "\n",
    "def build_semantic_documents_from_markdown(markdown: str, path: Path, base_metadata: Optional[dict] = None) -> list[Document]:\n",
    "    base_metadata = base_metadata.copy() if base_metadata else {}\n",
    "    documents: list[Document] = []\n",
    "    section_counter = 0\n",
    "    for section_counter, (heading, body) in enumerate(iter_markdown_sections(markdown), start=1):\n",
    "        if not body:\n",
    "            continue\n",
    "        title = heading or path.stem\n",
    "        for chunk_index, chunk in enumerate(chunk_section_text(body), start=1):\n",
    "            content = f\"{title}\n",
    "\n",
    "{chunk}\" if heading else chunk\n",
    "            metadata = {\n",
    "                \"source\": str(path.relative_to(PROJECT_ROOT)),\n",
    "                \"parser\": \"docling\",\n",
    "                \"section_title\": title,\n",
    "                \"section_order\": section_counter,\n",
    "                \"chunk_index\": chunk_index,\n",
    "            }\n",
    "            metadata.update(base_metadata)\n",
    "            documents.append(Document(page_content=content.strip(), metadata=metadata))\n",
    "    if not documents:\n",
    "        cleaned = markdown.strip()\n",
    "        if cleaned:\n",
    "            metadata = {\n",
    "                \"source\": str(path.relative_to(PROJECT_ROOT)),\n",
    "                \"parser\": \"docling\",\n",
    "                \"section_title\": path.stem,\n",
    "                \"section_order\": 1,\n",
    "                \"chunk_index\": 1,\n",
    "            }\n",
    "            metadata.update(base_metadata)\n",
    "            documents.append(Document(page_content=cleaned, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def convert_with_docling(path: Path) -> list[Document]:\n",
    "    base_metadata = {\"original_name\": path.name}\n",
    "    try:\n",
    "        result = DOC_CONVERTER.convert(str(path))\n",
    "        markdown = result.document.export_to_markdown()\n",
    "        return build_semantic_documents_from_markdown(markdown, path, base_metadata)\n",
    "    except UnicodeDecodeError:\n",
    "        fallback_doc = load_text_with_fallback(path)\n",
    "        base_metadata.update(fallback_doc.metadata)\n",
    "        return build_semantic_documents_from_markdown(fallback_doc.page_content, path, base_metadata)\n",
    "    except Exception as err:\n",
    "        print(f\"⚠️ Docling failed for {path.name}: {err}. Falling back to plain-text decoding.\")\n",
    "        fallback_doc = load_text_with_fallback(path)\n",
    "        base_metadata.update(fallback_doc.metadata)\n",
    "        return build_semantic_documents_from_markdown(fallback_doc.page_content, path, base_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse documents with Docling and build semantic chunks.\n",
    "semantic_docs: list[Document] = []\n",
    "for data_file in sorted(DATA_DIR.glob(\"*\")):\n",
    "    if data_file.suffix.lower() not in {\".pdf\", \".txt\", \".md\"}:\n",
    "        print(f\"Skipping unsupported file: {data_file.name}\")\n",
    "        continue\n",
    "    docs_from_file = convert_with_docling(data_file)\n",
    "    semantic_docs.extend(docs_from_file)\n",
    "    print(f\"Parsed {data_file.name} into {len(docs_from_file)} semantic chunks\")\n",
    "\n",
    "print(f\"Total semantic chunks: {len(semantic_docs)}\")\n",
    "if semantic_docs:\n",
    "    preview = {\n",
    "        doc.metadata.get(\"section_title\", doc.metadata.get(\"source\", \"unknown\")): len(doc.page_content)\n",
    "        for doc in semantic_docs[:5]\n",
    "    }\n",
    "    print(f\"Preview chunk lengths: {preview}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d9ea5",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Chunk Documents and Build the Vector Store\n",
    "\n",
    "Instead of fixed-length windows, we reuse Docling's structured Markdown output to group content by headings and logical paragraphs. The resulting chunks better follow semantic boundaries before they are embedded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b892f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# No additional chunking required; Docling already produced semantically-aware segments.\n",
    "chunked_docs = semantic_docs\n",
    "print(f\"Using {len(chunked_docs)} Docling-generated chunks for the vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ModelScope hosts multilingual embedding models; swap to a lighter model ID if resources are limited.\n",
    "embedding_model_id = \"damo/nlp_corom_sentence-embedding_zh-cn-base\"\n",
    "embeddings = ModelScopeEmbeddings(model_id=embedding_model_id)\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunked_docs, embedding=embeddings, collection_name=\"rag-data\")\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b59b91",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Choose an LLM Backend\n",
    "\n",
    "By default the notebook expects an OpenAI-compatible chat model. Uncomment the block that matches your setup:\n",
    "\n",
    "- **OpenAI:** requires `langchain-openai` and `OPENAI_API_KEY`.\n",
    "- **Ollama / other local models:** replace the cell with the appropriate LangChain chat wrapper (e.g. `from langchain_community.chat_models import ChatOllama`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ChatOpenAI is None:\n",
    "    raise ImportError(\"Install langchain-openai to use ChatOpenAI, or swap in another chat model.\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00762f87",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Build the Retrieval-Augmented Chain\n",
    "\n",
    "`RetrievalQA` wraps the retriever behind a simple `.invoke({\"query\": ...})` interface and returns both the answer and the supporting source documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64b658",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Ask Questions\n",
    "\n",
    "Use the helper below to query the knowledge base interactively. Set `show_sources=True` to inspect the retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320072ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask(question: str, show_sources: bool = True) -> None:\n",
    "    response = qa_chain({\"query\": question})\n",
    "    answer = response[\"result\"]\n",
    "    print(f\"\n",
    "Q: {question}\n",
    "A: {answer}\n",
    "\")\n",
    "    if show_sources:\n",
    "        print(\"Sources:\")\n",
    "        for idx, doc in enumerate(response.get(\"source_documents\", []), start=1):\n",
    "            source = doc.metadata.get(\"source\", \"unknown\")\n",
    "            snippet = doc.page_content[:200].replace(\"\n",
    "\", \" \") + (\"…\" if len(doc.page_content) > 200 else \"\")\n",
    "            print(f\"[{idx}] {source}\n",
    "    {snippet}\n",
    "\")\n",
    "\n",
    "# Example question (customize to your needs)\n",
    "ask(\"这份资料中提到的主要主题是什么？\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d87d81",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Next Ideas\n",
    "- Persist the `Chroma` index to disk (`persist_directory=...`) for faster reloads.\n",
    "- Swap in a chat-oriented chain (e.g. `ConversationalRetrievalChain`) to maintain dialogue history.\n",
    "- Attach tracing via `langsmith` or `langchain.debug` to inspect retrieval quality.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
