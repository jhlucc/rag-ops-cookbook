{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceeac48",
   "metadata": {},
   "source": [
    "\n",
    "# LangChain RAG Playground for the `data/` Directory\n",
    "\n",
    "This notebook sets up a minimal retrieval-augmented generation (RAG) workflow powered by LangChain. It ingests the resources stored in `../data` (a PDF research paper and a Chinese-language text file), builds a vector index, and exposes a helper function for interactive question answering.\n",
    "\n",
    "> ℹ️ Feel free to clone and extend these building blocks—swap in another LLM, persist the vector store, or wire the retriever into an application backend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a337b6",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "Uncomment and run the cell below if the required packages are not yet installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -q --upgrade langchain langchain-community langchain-openai langchain-text-splitters #     chromadb sentence-transformers pypdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9b735",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports and Configuration\n",
    "\n",
    "We keep paths relative so the notebook works whether it is launched from the project root or the `notebooks/` directory. The loader for the Chinese text file tries several common encodings before failing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# If you plan to use OpenAI, make sure langchain-openai is installed and set OPENAI_API_KEY.\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "except ImportError:\n",
    "    ChatOpenAI = None\n",
    "\n",
    "PROJECT_ROOT_CANDIDATES = [Path.cwd(), Path.cwd().parent]\n",
    "for candidate in PROJECT_ROOT_CANDIDATES:\n",
    "    data_dir = candidate / \"data\"\n",
    "    if data_dir.exists():\n",
    "        PROJECT_ROOT = candidate\n",
    "        DATA_DIR = data_dir\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not locate the 'data' directory. Update the path resolution logic above.\")\n",
    "\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n",
    "print(f\"Available data files: {[p.name for p in DATA_DIR.iterdir()]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea420b",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Load Raw Documents\n",
    "\n",
    "`load_text_with_fallback` tries multiple encodings because `data.txt` ships with non-UTF8 characters. Adjust or extend the encoding list if decoding still fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENCODING_CANDIDATES: Iterable[str] = (\"utf-8\", \"utf-8-sig\", \"gb18030\", \"iso-8859-1\")\n",
    "\n",
    "def load_text_with_fallback(path: Path, encodings: Iterable[str] = ENCODING_CANDIDATES) -> Document:\n",
    "    last_error: Optional[Exception] = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            text = path.read_text(encoding=enc)\n",
    "            metadata = {\"source\": str(path.relative_to(PROJECT_ROOT)), \"encoding\": enc}\n",
    "            return Document(page_content=text, metadata=metadata)\n",
    "        except (UnicodeDecodeError, LookupError) as err:\n",
    "            last_error = err\n",
    "    raise UnicodeDecodeError(\"Unable to decode\", str(path), 0, 0, str(last_error) if last_error else \"Unknown error\")\n",
    "\n",
    "# Load the PDF with PyPDFLoader so each page becomes a Document.\n",
    "pdf_path = DATA_DIR / \"2101.03697v3.pdf\"\n",
    "pdf_docs = []\n",
    "if pdf_path.exists():\n",
    "    pdf_loader = PyPDFLoader(str(pdf_path))\n",
    "    pdf_docs = pdf_loader.load()\n",
    "else:\n",
    "    print(\"⚠️ Skipping PDF because it was not found.\")\n",
    "\n",
    "# Load the text file with encoding fallback.\n",
    "text_path = DATA_DIR / \"data.txt\"\n",
    "text_docs = []\n",
    "if text_path.exists():\n",
    "    text_docs = [load_text_with_fallback(text_path)]\n",
    "else:\n",
    "    print(\"⚠️ Skipping text file because it was not found.\")\n",
    "\n",
    "raw_docs = pdf_docs + text_docs\n",
    "print(f\"Loaded {len(raw_docs)} documents\")\n",
    "print({doc.metadata.get('source', 'unknown'): len(doc.page_content) for doc in raw_docs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d9ea5",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Chunk Documents and Build the Vector Store\n",
    "\n",
    "The splitter keeps some overlap between chunks, which helps the retriever provide richer context windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b892f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
    "chunked_docs = text_splitter.split_documents(raw_docs)\n",
    "print(f\"Chunked into {len(chunked_docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SentenceTransformers models work well for multilingual text; change to a smaller model if resources are limited.\n",
    "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunked_docs, embedding=embeddings, collection_name=\"rag-data\")\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b59b91",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Choose an LLM Backend\n",
    "\n",
    "By default the notebook expects an OpenAI-compatible chat model. Uncomment the block that matches your setup:\n",
    "\n",
    "- **OpenAI:** requires `langchain-openai` and `OPENAI_API_KEY`.\n",
    "- **Ollama / other local models:** replace the cell with the appropriate LangChain chat wrapper (e.g. `from langchain_community.chat_models import ChatOllama`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ChatOpenAI is None:\n",
    "    raise ImportError(\"Install langchain-openai to use ChatOpenAI, or swap in another chat model.\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00762f87",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Build the Retrieval-Augmented Chain\n",
    "\n",
    "`RetrievalQA` wraps the retriever behind a simple `.invoke({\"query\": ...})` interface and returns both the answer and the supporting source documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64b658",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Ask Questions\n",
    "\n",
    "Use the helper below to query the knowledge base interactively. Set `show_sources=True` to inspect the retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320072ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask(question: str, show_sources: bool = True) -> None:\n",
    "    response = qa_chain({\"query\": question})\n",
    "    answer = response[\"result\"]\n",
    "    print(f\"\n",
    "Q: {question}\n",
    "A: {answer}\n",
    "\")\n",
    "    if show_sources:\n",
    "        print(\"Sources:\")\n",
    "        for idx, doc in enumerate(response.get(\"source_documents\", []), start=1):\n",
    "            source = doc.metadata.get(\"source\", \"unknown\")\n",
    "            snippet = doc.page_content[:200].replace(\"\n",
    "\", \" \") + (\"…\" if len(doc.page_content) > 200 else \"\")\n",
    "            print(f\"[{idx}] {source}\n",
    "    {snippet}\n",
    "\")\n",
    "\n",
    "# Example question (customize to your needs)\n",
    "ask(\"这份资料中提到的主要主题是什么？\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d87d81",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Next Ideas\n",
    "- Persist the `Chroma` index to disk (`persist_directory=...`) for faster reloads.\n",
    "- Swap in a chat-oriented chain (e.g. `ConversationalRetrievalChain`) to maintain dialogue history.\n",
    "- Attach tracing via `langsmith` or `langchain.debug` to inspect retrieval quality.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
