{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaIndex Enterprise RAG Workbook\n",
        "\n",
        "This notebook shows how to build a retrieval-augmented generation (RAG) pipeline over the assets in `data/` using LlamaIndex. The focus is on enterprise-friendly chunking, configuration hygiene, and reproducibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prerequisites\n",
        "\n",
        "- Python 3.11+ with access to this repository root\n",
        "- `pip install -r requirements-dev.txt` once the dependencies below are added\n",
        "- Secrets stored in `.env` (never commit credentials)\n",
        "\n",
        "> Run the install cell only when setting up a new environment or upgrading tooling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --quiet llama-index==0.10.54 python-dotenv pypdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure runtime\n",
        "\n",
        "### 2.1 Load environment configuration\n",
        "\n",
        "Secrets and deployment toggles live in `.env`. Update `LLM_PROVIDER` and relevant API keys before running the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "project_root = Path.cwd()\n",
        "if project_root.name == \"notebooks\":\n",
        "    project_root = project_root.parent\n",
        "\n",
        "dotenv_path = project_root / \".env\"\n",
        "if dotenv_path.exists():\n",
        "    load_dotenv(dotenv_path)\n",
        "    print(f\"Loaded environment from {dotenv_path}\")\n",
        "else:\n",
        "    print(\"No .env file found; relying on existing environment variables.\")\n",
        "\n",
        "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"openai\").lower()\n",
        "print(f\"Using LLM provider: {LLM_PROVIDER}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Configure LLM and embedding providers\n",
        "\n",
        "This block wires up default settings for OpenAI and can be extended for Azure or other providers. Failing fast when keys are missing keeps security audits simple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "if LLM_PROVIDER == \"openai\":\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        raise EnvironmentError(\"Set OPENAI_API_KEY before running index builds.\")\n",
        "    from llama_index.llms.openai import OpenAI\n",
        "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "    Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "elif LLM_PROVIDER == \"azure\":\n",
        "    if not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
        "        raise EnvironmentError(\"Set AZURE_OPENAI_API_KEY and related settings before running index builds.\")\n",
        "    from llama_index.llms.azure_openai import AzureOpenAI\n",
        "    from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "\n",
        "    Settings.llm = AzureOpenAI(\n",
        "        deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o\"),\n",
        "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-05-01-preview\"),\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    Settings.embed_model = AzureOpenAIEmbedding(\n",
        "        model=os.getenv(\"AZURE_OPENAI_EMBED_MODEL\", \"text-embedding-3-large\"),\n",
        "        deployment_name=os.getenv(\"AZURE_OPENAI_EMBED_DEPLOYMENT\", \"text-embedding\"),\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported provider '{LLM_PROVIDER}'. Extend this cell for your stack.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and profile source documents\n",
        "\n",
        "We target the curated corpora in `data/`. The reader handles both text and PDF assets, while metadata keeps traceability intact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.readers.file import PDFReader\n",
        "\n",
        "DATA_DIR = project_root / \"data\"\n",
        "assert DATA_DIR.exists(), f\"Expected data directory at {DATA_DIR}\"\n",
        "\n",
        "input_files = [\n",
        "    DATA_DIR / \"data.txt\",\n",
        "    DATA_DIR / \"2101.03697v3.pdf\",\n",
        "]\n",
        "\n",
        "reader = SimpleDirectoryReader(\n",
        "    input_files=input_files,\n",
        "    file_extractor={\".pdf\": PDFReader()},\n",
        ")\n",
        "\n",
        "documents = reader.load_data()\n",
        "for doc in documents:\n",
        "    file_name = doc.metadata.get(\"file_name\", \"unknown\")\n",
        "    word_count = len(doc.text.split())\n",
        "    print(f\"{file_name:<25} -> {word_count:>6} words\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Chunk documents with enterprise guardrails\n",
        "\n",
        "We apply `SentenceSplitter` to craft overlapping chunks so the downstream retriever keeps context without bloating tokens. Text and PDF assets can use different policies if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=600, chunk_overlap=120, paragraph_separator=\"\\n\\n\")\n",
        "\n",
        "txt_docs = [doc for doc in documents if doc.metadata.get(\"file_name\", \"\").endswith(\".txt\")]\n",
        "pdf_docs = [doc for doc in documents if doc.metadata.get(\"file_name\", \"\").endswith(\".pdf\")]\n",
        "\n",
        "txt_nodes = splitter.get_nodes_from_documents(txt_docs)\n",
        "pdf_nodes = splitter.get_nodes_from_documents(pdf_docs)\n",
        "\n",
        "print(f\"Text nodes: {len(txt_nodes)}\")\n",
        "print(f\"PDF nodes:  {len(pdf_nodes)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Inspect representative chunks\n",
        "\n",
        "Sampling nodes helps validate that entity boundaries and compliance-sensitive data stay intact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def describe_nodes(label, nodes, sample=2):\n",
        "    lengths = [len(node.text.split()) for node in nodes]\n",
        "    if not lengths:\n",
        "        print(f\"No nodes for {label}\")\n",
        "        return\n",
        "    avg_length = sum(lengths) / len(lengths)\n",
        "    print(f\"{label}: min={min(lengths)}, max={max(lengths)}, avg={avg_length:.1f} words\")\n",
        "    for idx, node in enumerate(nodes[:sample]):\n",
        "        print(f\"\\n--- {label} sample {idx + 1} ---\")\n",
        "        preview = node.text[:500]\n",
        "        print(preview)\n",
        "        if len(node.text) > 500:\n",
        "            print(\"...\")\n",
        "\n",
        "describe_nodes(\"data.txt\", txt_nodes)\n",
        "describe_nodes(\"2101.03697v3.pdf\", pdf_nodes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build the vector index and query engine\n",
        "\n",
        "We combine the chunked nodes into a vector index. Persist artifacts so production services can reload without rebuilding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "all_nodes = txt_nodes + pdf_nodes\n",
        "index = VectorStoreIndex(all_nodes, show_progress=True)\n",
        "\n",
        "query_engine = index.as_query_engine(similarity_top_k=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_question = \"Summarize the core idea discussed in the PDF and relate it to the dataset.\"\n",
        "response = query_engine.query(sample_question)\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Persist artifacts\n",
        "\n",
        "Persisted storage keeps builds deterministic across CI, staging, and production. Check large assets into object storage if they exceed Git limits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "persist_dir = project_root / \"storage\" / \"rag_llama_index\"\n",
        "persist_dir.mkdir(parents=True, exist_ok=True)\n",
        "index.storage_context.persist(persist_dir=persist_dir)\n",
        "print(f\"Persisted index to {persist_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quality controls and next steps\n",
        "\n",
        "- Add regression tests in `tests/` that call the query engine with fixtures from `tests/resources/`\n",
        "- Log `query_engine.retrieve` outputs during load testing to monitor drift\n",
        "- Wire up feedback loops (manual grading or eval harnesses) before promoting an agent to production\n",
        "- Rotate or encrypt `.env` secrets with your platform's secret manager\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}