{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbab64d",
   "metadata": {},
   "source": [
    "# HanLP语义切段示例\n",
    "\n",
    "本笔记基于 [HanLP](https://github.com/hankcs/HanLP) 的本地API，展示如何对中文长文本进行分句、分词、词性标注，并结合TF-IDF语义相似度完成按语义切段。所有代码都配有中文注释，便于了解每一步的处理逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963b4a0",
   "metadata": {},
   "source": [
    "## 1. 安装依赖（首次运行需要解开下面的pip命令）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b763f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -q hanlp scikit-learn pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfb4fb",
   "metadata": {},
   "source": [
    "## 2. 导入依赖并定位项目路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import statistics\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import hanlp\n",
    "from hanlp.utils.rules import split_sentence\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946090c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 允许 notebook 无论从项目根目录还是 notebooks/ 目录启动，都能找到 data 目录\n",
    "PROJECT_ROOT_CANDIDATES: Iterable[Path] = [Path.cwd(), Path.cwd().parent]\n",
    "for candidate in PROJECT_ROOT_CANDIDATES:\n",
    "    data_dir = candidate / \"data\"\n",
    "    if data_dir.exists():\n",
    "        PROJECT_ROOT = candidate\n",
    "        DATA_DIR = data_dir\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"未找到 data 目录，请检查项目结构或手动修改路径。\")\n",
    "\n",
    "print(f\"项目根目录: {PROJECT_ROOT}\")\n",
    "print(f\"可用数据文件: {[p.name for p in DATA_DIR.glob('*')]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b65b68",
   "metadata": {},
   "source": [
    "## 3. 读取并清洗原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c382c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PRIMARY_FILE = DATA_DIR / \"data.txt\"\n",
    "BACKUP_ENCODINGS = (\"utf-8\", \"utf-8-sig\", \"gb18030\", \"iso-8859-1\")\n",
    "\n",
    "last_error = None\n",
    "for enc in BACKUP_ENCODINGS:\n",
    "    try:\n",
    "        raw_text = PRIMARY_FILE.read_text(encoding=enc)\n",
    "        print(f\"使用编码 {enc} 成功读取文本，共 {len(raw_text)} 个字符\")\n",
    "        break\n",
    "    except UnicodeDecodeError as err:\n",
    "        last_error = err\n",
    "else:\n",
    "    raise last_error or UnicodeDecodeError(\"读取失败\", PRIMARY_FILE.name, 0, 0, \"无法解码文件\")\n",
    "\n",
    "# 保留段落结构，去除冗余空白字符\n",
    "clean_text = raw_text.replace('\r",
    "', ' ')\n",
    "clean_text = '\n",
    "'.join(line.strip() for line in clean_text.splitlines())\n",
    "clean_text = '\n",
    "\n",
    "'.join(block for block in clean_text.split('\n",
    "\n",
    "') if block.strip())\n",
    "print(f\"清洗后文本长度: {len(clean_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80c3cf",
   "metadata": {},
   "source": [
    "## 4. 使用 HanLP 进行分句与语法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6666c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 先用规则快速分句，减少模型负担\n",
    "sentences: List[str] = [s.strip() for s in split_sentence(clean_text) if s.strip()]\n",
    "print(f\"分句数量: {len(sentences)}\")\n",
    "print(\"示例句子:\")\n",
    "for sample in sentences[:3]:\n",
    "    print(\"-\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载HanLP多任务模型，获得分词、词性、依存等多种输出\n",
    "mtl_model = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\n",
    "analysis = mtl_model(sentences)\n",
    "\n",
    "print(\"HanLP返回的键:\", list(analysis.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445273b",
   "metadata": {},
   "source": [
    "## 5. 组织分词与词性结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49566203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将HanLP的输出整理成表格，便于后续特征计算\n",
    "records = []\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    tokens = analysis['tok/fine'][idx]\n",
    "    pos_tags = analysis['pos/ctb'][idx]\n",
    "    ner_tags = analysis['ner/ontonotes'][idx]\n",
    "    records.append({\n",
    "        \"句子编号\": idx,\n",
    "        \"原句\": sentence,\n",
    "        \"分词\": tokens,\n",
    "        \"词性\": pos_tags,\n",
    "        \"命名实体\": ner_tags,\n",
    "    })\n",
    "\n",
    "sentence_df = pd.DataFrame(records)\n",
    "sentence_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2188ee",
   "metadata": {},
   "source": [
    "## 6. 利用分词结果计算句子语义向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 只保留名词、动词、形容词等携带语义的词，提升语义聚合效果\n",
    "SEMANTIC_POS_PREFIXES = (\"N\", \"V\", \"JJ\", \"ADJ\")\n",
    "filtered_tokens: List[List[str]] = []\n",
    "for idx, row in sentence_df.iterrows():\n",
    "    tokens = [\n",
    "        token for token, pos in zip(row[\"分词\"], row[\"词性\"])\n",
    "        if any(pos.startswith(prefix) for prefix in SEMANTIC_POS_PREFIXES)\n",
    "    ]\n",
    "    filtered_tokens.append(tokens if tokens else row[\"分词\"])\n",
    "\n",
    "# 使用HanLP分出的词直接构建TF-IDF向量，避免重复分词\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    max_features=1024,\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(filtered_tokens)\n",
    "print(f\"TF-IDF矩阵维度: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba5679",
   "metadata": {},
   "source": [
    "## 7. 依据相邻句子的语义相似度生成语义段落"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 计算相邻句子的余弦相似度，低于阈值则划分新的语义段\n",
    "SIM_THRESHOLD = 0.25\n",
    "MIN_SENTENCE_PER_CHUNK = 1\n",
    "\n",
    "similarities = cosine_similarity(tfidf_matrix)\n",
    "chunk_spans: List[Tuple[int, int]] = []\n",
    "start = 0\n",
    "\n",
    "for idx in range(1, len(sentences)):\n",
    "    similarity = similarities[idx - 1, idx]\n",
    "    if similarity < SIM_THRESHOLD and (idx - start) >= MIN_SENTENCE_PER_CHUNK:\n",
    "        chunk_spans.append((start, idx))\n",
    "        start = idx\n",
    "\n",
    "# 处理最后一个片段，若不满足最小句数则并入上一段\n",
    "if len(sentences) - start >= MIN_SENTENCE_PER_CHUNK:\n",
    "    chunk_spans.append((start, len(sentences)))\n",
    "elif chunk_spans:\n",
    "    prev_start, _ = chunk_spans[-1]\n",
    "    chunk_spans[-1] = (prev_start, len(sentences))\n",
    "else:\n",
    "    chunk_spans.append((start, len(sentences)))\n",
    "\n",
    "print(f\"共划分语义段落: {len(chunk_spans)}\")\n",
    "chunk_spans[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3291050",
   "metadata": {},
   "source": [
    "## 8. 汇总每个语义段落的关键信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f175e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "chunk_records = []\n",
    "for chunk_id, (start, end) in enumerate(chunk_spans, start=1):\n",
    "    chunk_sentences = sentence_df.iloc[start:end]\n",
    "    chunk_text = \"\".join(chunk_sentences[\"原句\"].tolist())\n",
    "\n",
    "    # 聚合词性为名词或专有名词的高频词，辅助理解主题\n",
    "    candidate_terms = []\n",
    "    for tokens, pos_tags in zip(chunk_sentences[\"分词\"], chunk_sentences[\"词性\"]):\n",
    "        candidate_terms.extend(\n",
    "            token\n",
    "            for token, pos in zip(tokens, pos_tags)\n",
    "            if pos.startswith(\"N\") or pos.startswith(\"NR\") or pos.startswith(\"NT\")\n",
    "        )\n",
    "    top_terms = [term for term, _ in Counter(candidate_terms).most_common(5)]\n",
    "\n",
    "    chunk_records.append({\n",
    "        \"语义段编号\": chunk_id,\n",
    "        \"起始句\": start,\n",
    "        \"结束句\": end - 1,\n",
    "        \"段落句数\": end - start,\n",
    "        \"代表关键词\": top_terms,\n",
    "        \"段落文本\": chunk_text,\n",
    "    })\n",
    "\n",
    "chunk_df = pd.DataFrame(chunk_records)\n",
    "chunk_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff046970",
   "metadata": {},
   "source": [
    "## 9. 保存语义段结果以便后续复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "chunk_df.to_csv(output_dir / \"semantic_chunks.csv\", index=False)\n",
    "print(f\"语义切段结果已保存至 {output_dir / 'semantic_chunks.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c182f7b",
   "metadata": {},
   "source": [
    "## 10. 进一步扩展的方向\n",
    "\n",
    "- 调整 `SIM_THRESHOLD` 与特征词性列表，探索不同语义聚类颗粒度。\n",
    "- 利用 HanLP 的依存句法或语义角色标注结果，为段落生成摘要或结构化标签。\n",
    "- 替换 TF-IDF 为 HanLP 的语义相似度模型（如 STS 预训练模型），获得更精细的语义分组。\n",
    "- 将分段结果与上游 Docling 解析结合，构建多通道语义索引。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
