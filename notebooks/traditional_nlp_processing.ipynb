{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa17a38",
   "metadata": {},
   "source": [
    "# 传统NLP数据处理示例\n",
    "\n",
    "本笔记展示如何使用传统NLP工具对`data/`目录中的文本数据进行分词、清洗、特征提取以及关键词分析。所有代码均附带中文注释，便于团队成员快速理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138940d",
   "metadata": {},
   "source": [
    "## 1. 安装依赖（如已安装可跳过）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -q jieba scikit-learn pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e652ca",
   "metadata": {},
   "source": [
    "## 2. 导入库与路径设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import statistics\n",
    "from typing import Iterable\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 尝试定位项目根目录，确保无论从何处运行都能找到数据文件\n",
    "PROJECT_ROOT_CANDIDATES: Iterable[Path] = [Path.cwd(), Path.cwd().parent]\n",
    "for candidate in PROJECT_ROOT_CANDIDATES:\n",
    "    data_dir = candidate / \"data\"\n",
    "    if data_dir.exists():\n",
    "        PROJECT_ROOT = candidate\n",
    "        DATA_DIR = data_dir\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"未找到 data 目录，请确认项目结构是否正确。\")\n",
    "\n",
    "print(f\"项目根目录: {PROJECT_ROOT}\")\n",
    "print(f\"数据文件: {[p.name for p in DATA_DIR.glob('*')]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b0ae4",
   "metadata": {},
   "source": [
    "## 3. 加载与清洗原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PRIMARY_FILE = DATA_DIR / \"data.txt\"\n",
    "BACKUP_ENCODINGS = (\"utf-8\", \"utf-8-sig\", \"gb18030\", \"iso-8859-1\")\n",
    "\n",
    "# 逐个编码尝试读取文本，保证遇到异常编码时也能成功加载\n",
    "last_error = None\n",
    "for enc in BACKUP_ENCODINGS:\n",
    "    try:\n",
    "        raw_text = PRIMARY_FILE.read_text(encoding=enc)\n",
    "        print(f\"使用编码 {enc} 成功读取文本，共 {len(raw_text)} 个字符\")\n",
    "        break\n",
    "    except UnicodeDecodeError as err:\n",
    "        last_error = err\n",
    "        continue\n",
    "else:\n",
    "    raise last_error or UnicodeDecodeError(\"加载失败\", PRIMARY_FILE.name, 0, 0, \"无法解码\")\n",
    "\n",
    "# 使用正则去除多余的空白符与无意义符号，保留换行以帮助后续分析\n",
    "clean_text = re.sub(r\"[\t\r",
    "]+\", \" \", raw_text)\n",
    "clean_text = re.sub(r\"\\s+\n",
    "\", \"\n",
    "\", clean_text)\n",
    "clean_text = re.sub(r\"\n",
    "{2,}\", \"\n",
    "\n",
    "\", clean_text).strip()\n",
    "print(f\"清洗后文本长度: {len(clean_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2802676",
   "metadata": {},
   "source": [
    "## 4. 中文分词与词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载自定义停用词，避免无效词干扰分析\n",
    "STOPWORDS = {\n",
    "    \"的\", \"了\", \"和\", \"是\", \"在\", \"我们\", \"以及\", \"一个\", \"如果\", \"可以\", \"需要\"\n",
    "}\n",
    "\n",
    "# 使用结巴分词并同时进行词性标注\n",
    "words = []\n",
    "for word, flag in pseg.cut(clean_text):\n",
    "    word = word.strip()\n",
    "    if not word or word in STOPWORDS:\n",
    "        continue\n",
    "    words.append({\"词语\": word, \"词性\": flag})\n",
    "\n",
    "words_df = pd.DataFrame(words)\n",
    "print(f\"有效分词数量: {len(words_df)}\")\n",
    "words_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e98975",
   "metadata": {},
   "source": [
    "## 5. 构建TF-IDF特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f69cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义使用结巴的分词函数，供TF-IDF向量化器调用\n",
    "\n",
    "def jieba_tokenizer(text: str):\n",
    "    return [token for token in jieba.cut(text) if token.strip() and token not in STOPWORDS]\n",
    "\n",
    "# 因文本可能较长，我们按段落拆分后再计算TF-IDF\n",
    "paragraphs = [para.strip() for para in clean_text.split(\"\n",
    "\n",
    "\") if para.strip()]\n",
    "vectorizer = TfidfVectorizer(tokenizer=jieba_tokenizer, max_features=2000)\n",
    "tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"段落数量: {len(paragraphs)}，特征维度: {len(feature_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba918619",
   "metadata": {},
   "source": [
    "## 6. 抽取权重最高的关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 计算每个词语的平均TF-IDF权重，选择整体最重要的关键词\n",
    "\n",
    "tfidf_mean = tfidf_matrix.mean(axis=0).A1\n",
    "keyword_scores = list(zip(feature_names, tfidf_mean))\n",
    "keyword_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "TOP_K = 20\n",
    "print(f\"前 {TOP_K} 个关键词: \n",
    "\")\n",
    "for word, score in keyword_scores[:TOP_K]:\n",
    "    print(f\"{word:<20}\t{score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec60c7c",
   "metadata": {},
   "source": [
    "## 7. 计算基本文本统计指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 统计词长、词频、以及段落长度信息，帮助理解文本结构\n",
    "word_lengths = [len(w) for w in words_df[\"词语\"]]\n",
    "paragraph_lengths = [len(p) for p in paragraphs]\n",
    "\n",
    "stats = {\n",
    "    \"分词总数\": len(words_df),\n",
    "    \"唯一词语数量\": words_df[\"词语\"].nunique(),\n",
    "    \"平均词长\": statistics.mean(word_lengths),\n",
    "    \"中位词长\": statistics.median(word_lengths),\n",
    "    \"段落数量\": len(paragraphs),\n",
    "    \"平均段落长度\": statistics.mean(paragraph_lengths) if paragraph_lengths else 0,\n",
    "}\n",
    "\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae56333",
   "metadata": {},
   "source": [
    "## 8. 保存处理结果供后续使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 导出分词结果与关键词列表，方便其他流程复用\n",
    "words_df.to_csv(output_dir / \"data_words.csv\", index=False)\n",
    "pd.DataFrame(keyword_scores, columns=[\"词语\", \"平均权重\"]).to_csv(output_dir / \"keywords.csv\", index=False)\n",
    "\n",
    "print(f\"已保存分词结果与关键词至 {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56520f1a",
   "metadata": {},
   "source": [
    "## 9. 后续扩展建议\n",
    "\n",
    "- 更换停用词表以适配不同领域文本。\n",
    "- 引入自定义词典或行业术语，提升分词准确率。\n",
    "- 结合`n-gram`特征或`TextRank`算法挖掘更丰富的语义信息。\n",
    "- 将生成的CSV文件接入下游建模或知识库流程。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
